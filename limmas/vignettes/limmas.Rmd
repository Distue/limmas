---
title: "LIMMAS: Linear models for mass spectrometry"
author: "Thomas Schwarzl (<schwarzl@embl.de>), Version 0.4"
date: "2 September 2015" 
output:
  BiocStyle::html_document:
    toc: true
    toc_depth: 2
bibliography: bibliography.bibtex
---
<!--
%% \VignetteEngine{knitr::knitr}
-->

```{r, echo=FALSE} 
suppressPackageStartupMessages(library(BiocStyle))
```

```{r, echo=FALSE, eval=FALSE} 
# DESCRIPTION FILE needs
# VignetteBuilder: knitr
# Suggests: BiocStyle, knitr, rmarkdown
```

# Abstract
Quantitative mass spectrometry (MS) allows the analysis of quantitative high-throughput proteomics data. For each 
protein analysed, a mass spectrum is generated which allows protein identification and assessment of protein abundance levels. Despite the remarkable power of MS technology, finding quantitative differences among proteins levels in distinct physiological conditions remains a challenging task. This is especially true for data sets characterised by a high level of missing values, which is a characteristic frequently seen in MS output data sets [@Karpievitch2012]. 

Statistical software used for the identification of differentially expressed proteins typically require the data to be complete. Methods for dealing with missing data are usually inappropriate in that substantial amounts of information is lost and the data distribution is skewed; other, more efficient techniques present themselves as complicated to implement and data set-specific. 

Multiple imputation (MI) emerges as an interesting, easy to implement alternative for analysing incomplete data [@Rubin1987]. Multiple imputation is a Monte Carlo technique which substitutes missing values with *m >  1* estimated values (m typically amounts to 3-10), thereby producing *m > 1* complete data sets. Each one of these is individually analysed with the statistical method of choice and the results are then pooled into estimates, whereby missing-data uncertainty is incorporated [@VanBuuren2012] (http://sites.stat.psu.edu/~jls/mifaq.html).

In our approach, we show how incomplete protein entries can be salvaged using multiple imputation and the full amount of mass spec information subsequently used for differential protein expression analysis. Analysis of differential protein expression is  carried out with linear models and empiric Bayes method of the Linear Models for Microarray Data (`r Biocpkg("limma")`) package [@Smyth2005]. limma allows to fit a linear model to the expression data of each protein, whereby both simple experiments and experimental designs involving several groups and factors as well as time course experiments can be handled effectively.


# Introduction
## The problem with missing values

Due to techical reasons, (i) incomplete or incorrect peptide identification, (ii)
the presence of peptides that are below the detection limit of the instrument, or (iii)
the presence of low-abundance peptides [@Karpievitch2012], mass spectrometry data contains missing values. A typical data matrix one has to work with looks like this (missing values are denoted as NA): 

```
            GroupA     GroupA     GroupA    GroupB    GroupB   GroupB 
            SampleA    SampleB    SampleC   SampleD   SampleE  SampleF
ProteinA    10         NA          8        13        NA       NA
ProteinB    NA         4           NA       5         4        8
ProteinC    9          10          9        NA        NA       11
ProteinD    15         14          16       12        11       13
ProteinE    NA         NA          6        10        10       NA
ProteinF    14         13          14       NA        NA       NA
```

There are different strategies for working with such sparse data sets:

### Strategy 1: Use complete rows only

Only using complete rows would result in losing a data points, in fact 
whole features (proteins) would be removed. This problem increases with number of samples, since missing values can occur in any sample. In high dimensional samples most of the data set would be lost.

### Strategy 2: Fill in the missing values with averages of each group

If we would fill in the gaps with the average of each group, then every
generated data point would be exactly in the range of the measured of the 
other replicates. This would 'fake' precision and will ultimately add 
false confidence to your measurement resulting in wrong estimates and results.

### Strategy 3: Ignore missing values and compare average ratios

One can compute statistics with the values given and e.g. average over them and 
just build a ratio over the average. Again this adds a bias ignoring the real
variance of the feature and will not give any measurement of statistical significance.

### Strategy 4: Ignore missing values and do statistics

This approach would just do statistics ignoring missing values. In this approach, the variances will not be estimated correctly which will lead to wrong observations and results, especially in cases with few observations.

### Strategy 5: Impute values and fill up data 

Let's use statistics and fill up the gaps in the data set with estimated values. However it was shown that this increases noise [@Rubin1987] which will have an negatively impact on the results.

### Strategy 6: Multiple imputations

Multiple imputations was developed to cope with too small standard errors from imputes values [@Rubin1996,@VanBuuren2012]. By filling missing values multiple times (usually 3 to 10 times), analysing each filled up data set separately and then statistically combining (pooling) the individual results, the method outperforms all other strategies mentioned.

A typical workflow using multiple imputation looks like this:

```{r, eval=TRUE, echo=FALSE} 
plot(10, type="n", axes=F, xlab="", ylab="")

text(x = 0.9, y = 14, labels = "data with")
text(x = 0.9, y = 13.5, labels = "missing values")

text(x = 1.3, y = 12.8, font=3, labels = "multiple", cex=0.9)
text(x = 1.3, y = 12.3, font=3, labels = "imputation", cex=0.9)

lines(x = c(0.9, 0.7 ), y = c(13, 12))
text(x = 0.7 , y = 11.5, labels = "complete")
text(x = 0.7 , y = 11, labels = "data")

lines(x = c(0.9, 0.85), y = c(13, 12))
text(x = 0.85, y = 11.5, labels = "complete")
text(x = 0.85, y = 11, labels = "data")

lines(x = c(0.9, 1), y = c(13, 12))
text(x = 1, y = 11.5, labels = "...")
text(x = 1, y = 11, labels = "...")

lines(x = c(0.9, 1.1), y = c(13, 12))
text(x = 1.1, y = 11.5, labels = "complete")
text(x = 1.1, y = 11, labels = "data")


text(x = 1.3, y = 11, font=3, labels = "analyses", cex=0.9)
text(x = 1.3, y = 10.5, font=3, labels = "of individual", cex=0.9)
text(x = 1.3, y = 10, font=3, labels = "data sets", cex=0.9)

lines(x = c(0.7 , 0.7 ), y = c(10.5, 10))
text(x = 0.7 , y = 9.5, labels = "individual")
text(x = 0.7 , y = 9, labels = "result")

lines(x = c(0.85, 0.85), y = c(10.5, 10))
text(x = 0.85, y = 9.5, labels = "individual")
text(x = 0.85, y = 9, labels = "result")

lines(x = c(1, 1), y = c(10.5, 10))
text(x = 1, y = 9.5, labels = "...")
text(x = 1, y = 9, labels = "...")

lines(x = c(1.1, 1.1), y = c(10.5, 10))
text(x = 1.1, y = 9.5, labels = "individual")
text(x = 1.1, y = 9, labels = "result")


lines(x = c(0.7, 0.9), y = c(8.5, 7.5))
lines(x = c(0.85, 0.9), y = c(8.5, 7.5))
lines(x = c(1, 0.9), y = c(8.5, 7.5))
lines(x = c(1.1, 0.9), y = c(8.5, 7.5))

text(x = 0.9, y = 7, labels = "combined result")

text(x = 1.3, y = 8.1, font=3, labels = "pooling", cex=0.9)

```


In this package we provide a framework for multiple imputations. We implemented multiple imputations using the package Amelia II [@Honaker2011]. Users can use their own multiple imputation methods if wanted. 

The analysis and the statistical pooling are described in the next section.

## Analysis with linear models and multiple imputation

For the statistical analysis we use linear models on the 
*m* different imputed data sets. Linear models can be used 
for multifactorial designs which become more popular with the 
current developments in the MS field. Also, e.g. batch effects 
can be modeled in in the statistical analysis.

Here we make use of the fantastic
linear models for microarray (`r Biocpkg("limma")`) package. The 
analysis of MS data is very similar to microarray analysis, 
except the problem with the missing values in MS data. We cope 
with this issue using the multiple imputation approach. `r Biocpkg("limma")`
is extremely flexible, if you have no experience with `r Biocpkg("limma")`,
please go and have a look at their extremely well documented vignette. 

When analysing microarray data with the package `r Biocpkg("limma")`,
the workflow after preprocessing, filtering and transformation looks like this:

```{r, eval=TRUE, echo=FALSE} 
plot(10, type="n", axes=F, xlab="", ylab="")

text(x = 0.75, y = 14, labels = "raw data")
text(x = 1.1, y = 14, font=2, labels = "ExpressionSet", cex=1)

text(x = 1.1, y = 12.5, font=3, labels = "linear")
text(x = 1.1, y = 12, font=3, labels = "models")

lines(x = c(0.75, 0.75), y = c(13, 11))

text(x = 0.75, y = 10.5, labels = "fitted")
text(x = 0.75, y = 9.5, labels = "model")

text(x = 1.1, y = 10, font=2, labels = "MArrayLM", cex=1)

text(x = 1.1, y = 8.5,   labels = "significance", font=3)
text(x = 1.1, y = 8, labels = "testing", font=3)

lines(x = c(0.75, 0.75), y = c(9, 7))
text(x = 0.75, y = 6, labels = "results")
```

The raw data is stored as ExpressionSet class and linear models are fit to the raw data features resulting in a MArrayLM object. Significance testing is performed and returns usually a data frame containing p-value, adjusted p-values (corrected for multiple hypothesis testing) and log fold changes.

When introducing multiple imputation, the new workflow looks like this:

```{r, eval=TRUE, echo=FALSE} 
plot(10, type="n", axes=F, xlab="", ylab="")

text(x = 0.9, y = 14, labels = "raw data")
text(x = 1.3, y = 14, font=2, labels = "ExpressionSet", cex=0.8)

text(x = 1.3, y = 13.2, font=3, labels = "multiple", cex=0.9)
text(x = 1.3, y = 12.8, font=3, labels = "imputation", cex=0.9)

lines(x = c(0.9, 0.7 ), y = c(13.5, 12.5))
text(x = 0.7 , y = 12, labels = "complete")
text(x = 0.7 , y = 11.5, labels = "data")

lines(x = c(0.9, 0.85), y = c(13.5, 12.5))
text(x = 0.85, y = 12, labels = "complete")
text(x = 0.85, y = 11.5, labels = "data")

lines(x = c(0.9, 1), y = c(13.5, 12.5))
text(x = 1, y = 12, labels = "...")
text(x = 1, y = 11.5, labels = "...")

lines(x = c(0.9, 1.1), y = c(13.5, 12.5))
text(x = 1.1, y = 12, labels = "complete")
text(x = 1.1, y = 11.5, labels = "data")


text(x = 1.3, y = 12, font=2, labels = "MImputed", cex=0.8)
text(x = 1.3, y = 11.6, font=2, labels = "ExpressionSets", cex=0.8)

text(x = 1.3, y = 11, font=3, labels = "linear", cex=0.9)
text(x = 1.3, y = 10.6, font=3, labels = "models", cex=0.9)

lines(x = c(0.7 , 0.7 ), y = c(11, 10.5))
text(x = 0.7 , y = 10, labels = "fitted")
text(x = 0.7 , y = 9.5, labels = "model")

lines(x = c(0.85, 0.85), y = c(11, 10.5))
text(x = 0.85, y = 10, labels = "fitted")
text(x = 0.85, y = 9.5, labels = "model")

lines(x = c(1, 1), y = c(11, 10.5))
text(x = 1, y = 10, labels = "...")
text(x = 1, y = 9.5, labels = "...")

lines(x = c(1.1, 1.1), y = c(11, 10.5))
text(x = 1.1, y = 10, labels = "fitted")
text(x = 1.1, y = 9.5, labels = "model")

text(x = 1.3, y = 9.9, font=2, labels = "MImputed",       cex=0.8)
text(x = 1.3, y = 9.5, font=2, labels = "MArrayLM", cex=0.8)

lines(x = c(0.7, 0.9), y = c(9, 8))
lines(x = c(0.85, 0.9), y = c(9, 8))
lines(x = c(1, 0.9), y = c(9, 8))
lines(x = c(1.1, 0.9), y = c(9, 8))

text(x = 0.9, y = 7.5, labels = "combined model")

text(x = 1.3, y = 8.6, font=3, labels = "pooling", cex=0.9)

text(x = 1.3, y = 7.9, font=2,   labels = "Combined",   cex=0.8)
text(x = 1.3, y = 7.5, font=2, labels = "MArrayLM",   cex=0.8)

text(x = 1.3, y = 6.7,   labels = "significance", font=3, cex=0.9)
text(x = 1.3, y = 6.3, labels = "testing", font=3, cex=0.9)

lines(x = c(0.9, 0.9), y = c(7.0, 6.5))
text(x = 0.9, y = 6, labels = "results")
```

After using linear models on the *m* imputed data sets, the 
results have to be statistically *pooled*. This is important 
to adjust for in-between and across data set variablity.
Here, we included this functionality for 
easy pooling and significance testing of the results.

### Pooling 

The pooling is important to adjust for variablity within and between the
different imputed results. 

Once m imputed datasets have been generated, the model required for the full
dataset is fitted to each of the m imputed versions. This results in m model fits 
and m estimates of the parameter(s) of interest. Denote these estimates by ðœƒ!, ðœƒ!,â€¦, ðœƒ!.
Each fitted model also provides an estimate of the variance of the corresponding
ðœƒ!, ð‘— = 1,â€¦, ð‘›. For example, the model fitted to dataset 1 will provide an estimate
ðœƒ! for the parameter(s) and an estimate ðœŽ!
! for the variance of ðœƒ!. This results in
values for ðœƒ!, ðœƒ!,â€¦, ðœƒ! and corresponding variance estimates ðœŽ!
!, ðœŽ!
!,â€¦, ðœŽ!
! , which
are required for statistical inference. Once these values have been computed, they
then need to be combined to get the overall estimates.
The parameter estimates ðœƒ!, ðœƒ!,â€¦, ðœƒ! are averaged to get the final MI parameter
estimate:
ðœƒ!" =
1
ð‘›
ðœƒ!
!
!!!
However, we cannot simply average the variance estimates as there is now some
additional uncertainty since imputation has been used. Each imputed dataset will
be different (i.e. the imputed values will vary from dataset to dataset) and thus the
parameter estimates from the model fitted to each dataset will also vary. As a result,
there is variability within each imputed dataset (as quantified by the ðœŽ!
! values), but
there is also variability between the imputed datasets (i.e. there is additional
variability introduced due to the imputation). Simply averaging the ðœŽ!
! values only
provides information about the variability within imputations. Therefore it is
necessary to quantify and combine both the within and the between sources of
variability to get the appropriate overall variance value to be used for inference.
Within imputation variance ðœŽ!
! = !
! ðœŽ!
! !!
!!
Between imputation variance ðœŽ!
! = !
!!! (ðœƒ! âˆ’ !!
!! ðœƒ!")!


Total variance ðœŽ!"
! = 1 + !
! ðœŽ!
! + ðœŽ!
!
Confidence intervals and hypothesis tests can then be performed using ðœƒ!" and ðœŽ!"
!
instead of the individual results from each imputed dataset.


## Detection limit 

Missing values within a MS data set are the result of random or systematic
detection errors. It is therefore often hard to determine, especially for
proteins close to the detection limit, whether an entity is truly missing
or simply not recorded. 

Knowing how likely any protein entity is missing in a given sample is
therefore a useful insight that aids in the decision of pinpointing a
threshold for true negative detection. 

Replicates can be used to determine missing values introduced by random errors. 
Those can be filled up statistically with imputation methods as described above.
Then we are left with missing values due to the left cencoring caused by the
detection limit.

In biological, we often want to know if the protein is **higher** abundant 
in one group where it could not be detected. In this package we provide a 
method to first estimate a detection limit and then calculate significance for
differential expression higher than the given detection limit.


# Input data


| Generic Input  | Specfic Input |
| ------------- |------------- | 
| 1. Intensity Matrix    | 1. MaxQuants proteinGroups.txt   | 
| 2. Feature Data (Additional info to the features)  | 2. Pheno Data (Sample Meta Data) |
| 3. Pheno Data (Sample Meta Data / Sample Information)  | |  
| **-> as ExpressionSet**   | **-> limmas conversion to ExpressionSet** |  



LIMMAS takes an object of class *ExpressionSet*, here called *eset*, as input. Basically, an *ExpressionSet* is an object containing expression values as well as the sample annotation (pheno) and feature annotation (protein or peptide information). Those are stored in slots of an *ExpressionSet*. If you have questions regarding the *ExpressionSet* class, please read the Bioconductor help pages.

Here is an overview of the slots used in *ExpressionSet*

The pheno data stored in *eset* can be accessed with *pData(eset)* and should describe the experimental setup. *exprs(eset)* provides the protein intensities, *annotation(eset)* the annotation for the proteins, and *fData(eset)* the meta-information about the quantified proteins.

LIMMAS provides functions to create an object of ExpressionSets from tab-separated files or \Robject{data.frames} from any software for MS quantification. 

Currently, functions for the processing of output files by the popular software *MaxQuant* are provided. LIMMAS directly reads *proteinGroups.txt* files as input, if the appropriate pheno data is provided manual.

## Example data
 
To demonstrate our approach, we apply the functions in the LIMMAS package to a label-free quantitative (LFQ) MS data comparing the protein abundance of immunoprecipitated GEFH1 binding partners. 2 technical and 3 biological 
replicates each of sample groups were generated. The groups are: (1) antibody control, (2) GEFH1, and (3) GEFH1 + Mek Inhibitor. These were quantified using \texttt{MaxQuant} software (Ref) and technical replicates were also summed up by \texttt{MaxQuant}. Activation of the guanine exchange factor GEFH1 is dependent on the action of Mek (ref). This study aimed at detecting changes in protein binding dependent on the presence of a specific Mek Inhibitor.

The ExpressionSet containing this data can be loaded with

```{r} 
suppressPackageStartupMessages(require(limmas))
suppressPackageStartupMessages(library(knitr))

data(gefh1inhib)
data <- gefh1inhib
pheno <- gefh1inhib.pheno
kable(pData(pheno))
```

If you want to use the example data, you can continue to section [Filtering].


## MaxQuant data

Firstly, sample information called 'pheno file' specifying the experimental conditions and different factors of the study is needed as *SmartAnnotatedDataFrame*. This is then used to read in the raw protein intensities from the **MaxQuant** output *proteinGroups.txt* file. This will provide an *ExpressionSet* for further analysis.

### Pheno data

Here we show how to create a pheno file and read in pheno information
as *SmartAnnotatedDataFrame*

A pheno file is a tab-separated text-file where each row describes a sample.
As our beloved biologists often chose variable names, **limmas** contains
the functionality to rename these samples to a bioinformatics-friendly name.

In our example, the rows start with the sample name,
then the column name in the raw data file where the
protein intensities can be found. The next column 'groups' 
describes the sample groups. Any other 
factors for the analysis can be added, one factor per column, 
like batch, time.

We call this pheno file *pheno.txt*.

```
                  OrigNames               groups       batch 
Control.A           LFQ.intensity.ALEX_1	 Control     A
Control.B	        LFQ.intensity.ALEX_2	 Control     B
Control.C	        LFQ.intensity.ALEX_3	 Control     C
GEFh1.A             LFQ.intensity.ALEX_4	 Gefh1       A
GEFh1.B             LFQ.intensity.ALEX_5	 Gefh1       B
GEFh1.C             LFQ.intensity.ALEX_6	 Gefh1       C
GEFh1.MEKInhib.A	 LFQ.intensity.ALEX_7	 Gefh1Inhib  A
GEFh1.MEKInhib.B	 LFQ.intensity.ALEX_8	 Gefh1Inhib  B
GEFh1.MEKInhib.C	 LFQ.intensity.ALEX_9	 Gefh1Inhib  C
```

#### Reading from a file

Your pheno file can be directly read into the workspace by calling

```{r, eval=FALSE, echo=TRUE}
require(limmas)
pheno <- read.pheno("pheno.txt", originalNamesCol="OrigNames", sampleNamesCol="")
```

When reading in the protein intensity raw data, it is often convenient to re-name the samples. LIMMAS provides this function automatically when given original sample names and new sample names for renaming.

*originalNamesCol* defines the name of the column 
of the pheno data which keeps the raw sample names. This are the exact column names of the data file which hold the protein intensities and are read in in the next step. In the case of the here described data set, this would be for example ``OrigNames''. 

*sampleNamesCol* specifies the name of the column of the pheno data
which keeps the new sample names. Empty quotes, "",  simply means, that the sample names are stored in the 
rownames.

In short, after reading the data file, the name for sample "LFQ.intensity.ALEX_1" will be "Control.A" in the resulting *ExpressionSet*. Should the user want to keep the original sample names, the same value must be specified in both arguments.

*read.pheno* creates pheno data of the class *SmartAnnotatedDataFrame*.

#### SmartAnnotatedDataFrame functions 

*SmartAnnotatedDataFrame* is a derived class from *AnnotatedDataFrame* and therefore has all functionalities AnnotatedDataFrame objects, as well as additional functions.

An already existing method pData displays the raw pheno data.

```{r, eval=TRUE}
kable(pData(pheno))
```

Sometimes samples have to be excluded because of quality issues, or because only a subset of the data set is analysed. In the *SmartAnnotatedDataFrame* object, factors will automatically re-level when samples are excluded. If for example, in the example data the control was to be excluded,

```{r, eval=FALSE}
# TODO
pheno.without.control <- pheno[,-c(1:3)]
```

the factor returned by pData(pheno.without.control)[,"groups"] would contain two, not three levels

```{r, eval=FALSE}
# TODO
pData(pheno.without.control)[,"groups"]
```

When successfully created an \Robject{SmartAnnotatedDataFrame} following functions will work:

*getOriginalNames* returns all original sample names.

```{r, eval=TRUE}
getOriginalNames(pheno)
```

*getSampleNames* returns all new sample names.
```{r, eval=TRUE} 
getSampleNames(pheno) 
```

*getAnnotatedDataFrame* returns an AnnotatedDataFrame without the original sample name column and the new samples as rownames. 
```{r, eval=TRUE}
class(getAnnotatedDataFrame(pheno))
```

##### Protein intensity data from MaxQuant proteinGroups.txt 

When the *SmartAnnotatedDataFrame* is provided, the input of **MaxQuant** intensity data is very easy.

The **MaxQuant** text output file *proteinGroups.txt* contains the intensities on protein level and a lot of meta information about the quantification process. *read.maxQuant* takes an *SmartAnnotatedDataFrame* and reads in the samples specified in the *SmartAnnotatedDataFrame*. It returns the *ExpressionSet* object we need for analysis.

```{r, eval=FALSE, echo=TRUE}
require(limmas)
data <- read.maxQuant(file = 'proteinGroups.txt', pheno, splitIds = ';')
```

If you want to work with the data now, please continue at section **Filtering**. 

##### Protein intensity from data frame

The function *createExpressionSetFromMaxQuant* processes MaxQuant output and provides an object of the class *ExpressionSet* as utilised in the package `r Biocpkg("Biobase")`.
The function splits the data.frame in the tab-separated data file using the original sample name columns from the *SmartAnnotatedDataFrame* object as protein intensities in *exprs(data)*. It assigns the sample names defined in pheno as column name of the intensities. Via the parameter protein annotation is retrieved from the data and stored in *annotation(data)*. The remaining input data is stored in *fData(data)*. This can contain meta-information about peptide 
counts, contaminants, etc. 

In summary, the features of data can be viewed as follows:

* *exprs(data)* returns the protein intensity values 
* *pData(data)* returns the corresponding pheno data
* *annotation(data)* views the protein ids
* *fData(data)* views other features included in the raw input experimental data 

### Other sources

For sources different from *MaxQuant*, *ExpressionSet* objects can be created easily with the provided function

```{r, eval=FALSE, echo=TRUE}
require(limmas)
data <- createExpressionSet(intensities, pheno, features, annotation)
```

* *intensities* is an expression matrix of protein intensities.
* *pheno* can be either an *AnnotatedDataFrame* or *SmartAnnotatedDataFrame*.      
* *features* is *matrix* for feature data.
* *annotation* is a *character* string containing the protein annotation.

For manual creation of an *ExpressionSet*, please refer to the *ExpressionSet* Bioconductor pages.

# Filtering

Three filtering steps strip the raw data from unwanted protein entries, including proteins identified on the basis of only one peptide fragment and positive protein hits in reverse- and contaminant databases. This information should be found in *fData(data)*.

## Peptide Filter

*peptideFilter* represents the first filtering step. This function removes all protein entries of an object of the class *ExpressionSet*, which are characterised by a peptide count smaller than or equal to a user-defined cut-off in the column named "peptides". This column contains the maximum amount of peptides for given proteins found in any sample. *peptideColumns* specifies in which columns the peptide information is stored, that can be a single or multiple columns. *method* defines if the peptide count has to '''greater''' the
*peptideCutoff* in "all" or in "any" of the columns specified in *peptideColumns*.

```{r, eval=TRUE}
nrow(data)
data <- peptideFilter(data, peptideCutoff = 1, peptideColumns = c("Peptides"), method = "any")
nrow(data)
```

The default value for the parameter *peptideCutoff* within the function is 1. The amount of protein entries after filtering can easily be checked using *nrow(data)*.

## Reverse Filter

*reverseFilter* is a function responsible for the removal of all proteins shown to be positive in the reverse peptide database of choice. This is annotated in the data set with e.g. a '+' symbol in the column 'reverse'. Should the original data set contain a different symbol to indicate positive hits, the user is required to change the symbol description with the parameter 'symbol' within the function.

```{r, eval=TRUE}
data <- reverseFilter(data, symbol = "+", reverseColumn = "Reverse")
nrow(data)
```

## Contaminant Filter

Finally, the *contaminantFilter* function removes proteins contained in the chosen contaminant database. The same remark concerning the symbol used to indicate positive hits in the column 'contaminant' is used as above.

```{r, eval=TRUE}
data <- contaminantFilter(data, symbol = "+", contaminantColumn = "Contaminant")
nrow(data)
```


# QC before preprocessing

## Complete Rows

Of great interest for the user and for the purpose of MI is the degree of completeness of the data set. The number of missing value-free rows across samples can  be determined by calling *checkCompleteRows(data)*

```{r, eval=TRUE}
complete.before.imputation <- checkCompleteRows(data)
complete.before.imputation
```

In contrary, the median missingness across the data set provides a crude overview of missing data in each sample. This can be checked with *checkMissingness(data)*:

```{r, eval=TRUE}
checkMissingness(data)
```

A better overview is provided using the visual representation of missingness will be later used in *Detection limit*. Before this, we will preprocess the data.

## Histograms

With \Rfunction{hist} we can create a histogram of the non-missing values before normalisation. We want to (log) transform the data however for the visualisation.

```{r, eval=TRUE}
require(affyPLM)
hist(transformData(data), lwd = 2, main = "Before Normalisation")
```

## Boxplots

*boxplot* will create a barchart for our data
set before the preprossing.

```{r, eval=TRUE}
boxplot(transformData(data), lwd = 2, main = "Before Normalisation", las = 2)
```

# Preprocessing

Before imputation and statistical testing, the data is subjected to normalization, scaling and transformation.
It has been observed that normalization is best carried out prior to MI (Ref). Normal data distribution 
is necessary for imputation and fitting the linear model in a later step. Quantile normalization is set as the default method. 

```{r, eval=TRUE}
data.normalized <- normalizeData(data,
                                 minIntensity = 0,
                                 FUN = normalize.ExpressionSet.quantiles) 
```

The paramenter minIntensity is arbitrary and allows to set a cut-off for protein intensity values, below which the user deems the observed values to be inaccurate, e.g. due to known irregularities of the mass spectrometer.
Alternative Normalization methods can be used and suggested functions for ExpressionSet objects are described 
in the package `r Biocpkg("affyPLM")` and includes:

* normalize.ExpressionSet.quantiles
* normalize.ExpressionSet.loess
* normalize.ExpressionSet.contrasts
* normalize.ExpressionSet.qspline
* normalize.ExpressionSet.scaling

The above functions can be employed by passing them into the *normalizeData* function call with the parameter. Remember that you can use the full functionality of an *ExpressionSet* object. 
 
Eventually, you want to subset the data to normalize the controls and samples 
separately. The use of *ExpressionSet* in **limmas** provides a very flexible
data structure to adapt to different data set properties.

The function *scaleData* scales down the protein intensity values by dividing them by a user-defined scalefactor argument (here chosen to be 1000).

```{r, eval=TRUE}
data.scaled <- scaleData(data.normalized, 1000)
```

Normal distribution usually is achieved via a final step of log-transformation, which is widely used in mass spectrometry data. 

```{r, eval=TRUE}
data.transformed <- transformData(data.scaled)
```

Other transformations can be passed as functions into the *transformData* function if needed with the parameter FUN. Here is the example how the log transformation is passed on. In short, this function will be simply applied to the ExpressionSet.

```{r, eval=FALSE, echo=TRUE}
data.transformed <- transformData(data.scaled,
                                  FUN = function(x) {
                                           exprs(x) <- log(exprs(x));
                                           return(x)
                                   })
```

*ExpressionSet* has very useful functionality: e.g. we can plot histograms and boxplots of the normalized data

```{r, eval=TRUE}
hist(data.transformed, lwd = 2, main = "Normalised and scaled data") 
```

```{r, eval=TRUE}
boxplot(data.transformed, lwd = 2, main = "Normalised and scaled data", las = 2)
```

# QC after preprocessing

## Normality

Normality is checked using normal quantile-quantile (Q-Q) plots. A 45 degree diagonal from left bottom to top 
right would mean a perfectly normal distributed sample as shown in Figure below.
Alternatively, a Shapiro-Wilk test could be performed.

```{r, eval=TRUE}
par(mfrow=c(3,3), oma=c(3,3,3,3))
for(i in 1:9) {
   qqnorm(exprs(data.transformed)[,i],
          main = paste("Normal Q-Q Plot: ", 
          colnames(exprs(data.transformed))[i], sep = ""),
          cex=1,
          cex.main=1.4)
}
par(mfrow=c(1,1), oma=c(1,1,1,1))
```


## Detection limit

Therefore we estimate
the percentage of non-detection with following method.

```{r, eval=TRUE}
dev.null <- lapply(levels(pData(data.transformed)[,"groups"]), function (x) { 
   plotNAdensity(data.transformed, group = x, groupCol = "groups")
})  
```

This can be accomplished utilising the function *getMaxProbabilityMissingByChance(data, "groups", 3)*.
This command calculates the maximum probability of any number of observed values being missing by chance in all groups of replicates, whereby the argument â€˜groupsâ€™ determines the column in the pheno file that defines individual groups; the number of missing values can be specified with a number \geq number of replicates.

```{r, eval=FALSE}
#TODO
getMaxProbabilityMissingByChance(data, "groups", 3)
```

These plots help us to understand the behaviour of features with missing 
values: 

This plot shows amount of missing values versus the median expression 

```{r, eval=TRUE}
plotMedianVsNAs(data.transformed, "Gefh1", groupCol = "groups")
```

shows the standard deviation veersus the median expression

```{r, eval=TRUE}
plotMedianVsSD(data.transformed, "Gefh1", groupCol = "groups")
```

This plot shows the standard deviation versus the amount of missing values

```{r, eval=TRUE}
plotNAsVsSD(data.transformed, "Gefh1", groupCol = "groups")
```


Now we estimate the detection limit for all groups under the assumption
that these samples were run on the same machine.


%
%
%
%
% TODO ESTIMATE LIMITS
%
%
%

This function estimates 


# Multiple Imputation

Multiple Imputation (MI) is a statistical method to fill up missing data {Ref}.
Compared to other imputation methods, it has the advantage that the imputated
values (statistically generated filled in gaps) reflect the variance of the 
real data set {Ref}. This is in particular important for high-throughput 
experiments with replicates.

**limmas** uses the `r CRANpkg("Amelia II")` R package for the multiple imputation,
however it can be substituted for any multiple imputation algorithm can be used.
The input is an *ExpressionSet* object and the output is an
*MImputedExpressionSet* object. The *MImputedExpressionSet* object
contains a list of imputed data sets (*ExpressionSet*s with filled up
values), as well as the original data.
 
## Amelia II - Independent Groups

In a immunoprecipitation setup with quantitative label-free MS,
protein binding partners of a specific target are investigated.
In different conditions, a protein might have a strong binding
affinity, whereas in other conditions a protein does not bind
at all. 

We observed, that MI algorithms tend to fill up values for 
features in a condition with no observerations at all. This 
independent group approach was developed to firstly take 
replicate information for filling up missing values and 
secondly leave empty observations untouched for biological
interpretation.

MI of the incomplete data set is carried out utilising the
function *imputeIndependentGroupsWithAmelia*. As 
described in the `r CRANpkg("Amelia II")` package (Ref), this
command imputes the missing values present in the data set
on the basis of the variation of the experimental values
recorded, thereby producing a user defined number
*m \textgreater 1* of complete data sets. The default number
of imputations is set to *m = 10*. 

```{r, eval=TRUE}
impResult <- imputeIndependentGroupsWithAmelia(data.transformed,
                                         minTotalPresent = 2,
                                         groupingCol = "groups",
                                         quiet = T)
```


*groupingCol* specifies the sets of replicates (groups)
across which imputation is to be carried out, as seen in pheno.
Importantly, the parameter *minTotalPresent* specifies
the number of observed values required by the user in each set
of replicates (group). The following rationale underlies this
step: For any specific protein and for each group, if *number
of observed values >=  minTotalPresent*, then both the present
and missing values remain unaltered and can be imputed. If however
*number of observed values < minTotalPresent*, all values for
that protein in that particular group are replaced by NA values 
and can therefore not be imputed. This approach aids the imputation
of likely true positives while hindering the propagation of likely 
false positive observations.

*Note:* **Amelia II** *will raise 
a warning 'There are observations in the data that are completely 
missing. These observations will remain unimputed in the final 
datasets.' which is exactly what we want.*

In the example data, we decided that of the three replicates in 
each group at least two were required to contain observed data points, 
in order for the protein intensity values in that specific group to 
be imputed. Contrarily, for protein entries where only one value out
of three was observed in a group and two were missing, the observed 
value was regarded as a likely false positive and was replaced by NA,
thereby completing a full set of three NAs. These data points cannot
be imputed.

The resulting object **impResult** is a list comprising an object of
class **MImputedExpressionSets**, which is specific to **LIMMAS** 
and carries the imputed protein expression values, and a list containing
further imputation parameters returned by **Amelia**.

```{r, eval=TRUE}
summary(impResult)
```

At this point the in-build **Amelia II** quality checks 
can be performed. Below is an example of overdispersed starting 
values diagnostics, specifically for the imputed data in the control
group. This diagnostic tool gives some indication as to how sensitive 
the imputation process is for any particular data set, by visualising
how the first and second (as dims=2) principal component change 
across m imputations (here *m = 10*). If print to screen 
(*p2s*) is set to 2, a diagnostic output is created. Please 
refer to the **Amelia II** package description for further 
details; however, if all lines in the overdispersed starting value
plot converge at the same point, it is reflective of a well behaved
likelihood of the data as seen here:

```{r, eval=TRUE}
disperse(impResult[["imputation"]]$Control, m = 10, dims = 2, p2s = 2)
```
Overdispersed starting values

The \Rclass{MImputedExpressionSet} can be extracted from
\Robject{impResult} with this command:

```{r, eval=TRUE}
data.imputed <- impResult[["data"]]
```

The features of the \Rclass{MImputedExpressionSet} in data.imputed
can be explored as follows:

- Number of Imputations (returns 10, if set to default)
```{r, eval=TRUE}
numberImputations(data.imputed)
```

- Imputed values of all groups for the first round of imputation
```{r, eval=TRUE}
kable(head(intensities(data.imputed, 1)))
```

- Shows the expression set data for the first round of imputation:
```{r, eval=TRUE}
class(eset(data.imputed, 1))
```

- Shows the pheno file
```{r, eval=TRUE}
kable(pData(data.imputed))
```

- Annotation of the imputed data set
```{r, eval=TRUE}
head(annotation(data.imputed))
```

- All feature data
```{r, eval=TRUE}
kable(fData(data.imputed)[1:5,1:5])
```

- Dimensions of the data matrix returned by the first round of imputation 

```{r, eval=TRUE}
dim(eset(data.imputed, 1))
```

- number of rows
```{r, eval=TRUE}
nrow(data.imputed)
```

## Amelia II - no groups

*limmas* will be extended in the future to include other imputation methods 
and variants. If you want to impute over the whole data set without 
group associations, then just insert the same value for all samples in the 
group column in the *pheno* file and run Amelia II - Independent groups - 
as explained above.

## Other imputation methods

As mentioned, custom imputation algorithms can be used. Just transform the 
*m* imputed data sets into a list of *ExpressionSets* to create an 
*MImputedExpressionSet*.

```{r, eval=FALSE, echo=TRUE} 
data.imputed <- MImputedExpressionSet(list.of.ExpressionSets)
```

After the successful imputation, the analysis will be performed.

# Before analysis

## Bait Control

In immunoprecipitation setups, you want to check for the protein of interest. 
Usually, anti-body controls are performed to control for unspecific binding.
The easiest way to quickly assess the levels of the protein is to plot the
protein intensities in the MS data.

```{r, eval=TRUE}
plotExpression(data.imputed, "Q92974")
```


Here, clearly you can see that the control was detected in all of the samples
and the control and has a much higher expression than the control in both 
conditions.


## Extract complete cases

There are applications where you want to compare only complete feature rows in
your data set, or if you want to assess how many rows got completely filled 
up by the imputation. The complete feature rows can be extracted by:

```{r, eval=TRUE}
data.complete <- completeCases(data.imputed)
```

and the row count can be checked by
```{r, eval=TRUE}
complete.after.imputation <- nrow(data.complete)
complete.after.imputation
```

Now let's show the number of features which are now fully filled up by multiple imputation:
```{r, eval=TRUE}
complete.after.imputation - complete.before.imputation
```

Currently, we are improving the statistical power for analysis of these proteins. We are now continuing the analysis for the complete cases only and then will show you how to use the full information of the data set.

# Analysis



## The design matrix

A design matrix has to be constructed for fitting the model. This
basic concept is well documented in the `r Biocpkg("limma")` vignette.
Here an example is giving where the model is fitted on group and
batches.

```{r, eval=TRUE}
p      <- pData(data.imputed)
design <- model.matrix(~0 + p$groups + p$batch, data=p) 
colnames(design) <- c(levels(p$groups), levels(p$batch)[-1])
```


## Statistics for complete data after imputation

Now we fit the model for each imputed data set, do the statistics, pool the results, do 
the significance testing, multiple hypothesis testing correction,
and display the top results from the first contrast - Gefh1 versus Control -
in `r Biocpkg("limma")` fashion.

```{r, eval=TRUE}
contrasts <- c("Gefh1 - Control",
               "Gefh1Inhib - Control", 
               "Gefh1Inhib - Gefh1")
fit <- combineFits(contrastFit(limmasFit(data.complete, design), contrasts))
kable(topTableImpute(fit, n=5, coef=1))
```

To retrieve the significant results for all different contrasts use: 

```{r, eval=TRUE}
res.complete <- list()
res.complete[["gefh1"]]          <- getSignificantFeatures(fit, coef=1, p.value=0.05)
res.complete[["gefh1inhib"]]     <- getSignificantFeatures(fit, coef=2, p.value=0.05)
res.complete[["inhib.vs.gefh1"]] <- getSignificantFeatures(fit, coef=3, p.value=0.05)

lapply(res.complete, nrow)
```

Since we have control samples, we want to filter, the results for significantly
higher expression and \textgreater 0.5 logFC in samples versus controls.

```{r, eval=TRUE}
res.complete.filter <- unique(unlist(lapply(c("gefh1", "gefh1inhib"), function(x) {
   rownames(res.complete[[x]][res.complete[[x]][,"logFC"] > 0.5,])
})))

sig.complete <- res.complete[["inhib.vs.gefh1"]][
                       rownames(res.complete[["inhib.vs.gefh1"]]) 
                                                    %in% res.complete.filter,]
nrow(sig.complete)
```

```{r, eval=TRUE} 
kable(head(sig.complete))
```

Four proteins are significantly expressed between the two conditions
as well as compared to the control in the complete data set. We plot 
the top result:

```{r, eval=TRUE}
plotExpression(data.complete, as.numeric(rownames(sig.complete[1,])))
```

When a sample contains multiple data points, this means it was imputed.
Then it did not have any values but it was filled up by the 
multiple imputation m times. Other samples with only one data point have
real values.

This approach is **not** desirable, because although many data points 
were filled-in with multiple imputation, it implies losing many 
valid features, totally ignoring valid input data. Similar removal of incomplete
data is used in many statistical data packages out there. The next section will
explain the analysis of the whole data set using **all** available data.

## Statistics for whole data set after imputation

The analysis is done in two steps. First, we will use all features 
for building the linear model in `r Biocpkg("limma")` while ignoring missing
data. Then a secondary approach uses an estimated detection limit 
to select features below the detection limit which have real values
in another sample group. 

This is explained easily with an example. As above, we create a linear model 
for all imputed data sets, do hypothesis testing, pooling, and multiple 
hypothesis correction for all features.

*Note: Because of the missing values,* `r Biocpkg("limma")` *can return 
Warnings which are expected which can be ignored.*

```{r, eval=TRUE}
fit <- suppressWarnings(limmasFit(data.imputed, design))
fit <- combineFits(contrastFit(fit, c("Gefh1 - Control",
                                      "Gefh1Inhib - Control", 
                                      "Gefh1Inhib - Gefh1")))
res.full <- list()
res.full[["gefh1"]]         <- getSignificantFeatures(fit, coef=1, p.value=0.05)
res.full[["gefh1inhib"]]    <- getSignificantFeatures(fit, coef=2, p.value=0.05)
res.full[["inhib.vs.gefh1"]]<- getSignificantFeatures(fit, coef=3, p.value=0.05)

lapply(res.full, nrow)
```


Now we want statistically test for 

```{r, eval=FALSE}
res.limit[["gefh1"]] <- featuresBelowLimit(data.imputed, "Gefh1", "Control",
                                                    limitLogFC = 1, limitRate  = 0.05)
res.limit[["gefh1inhib"]]     <- featuresBelowLimit(data.imputed, "Gefh1Inhib", "Control",
                                                    limitLogFC = 1, limitRate  = 0.05)
res.limit[["inhib.vs.gefh1"]] <- featuresBelowLimit(data.imputed, "Gefh1Inhib", "Gefh1",
                                                    limitLogFC = 1, limitRate  = 0.05)

lapply(res.limit, nrow)
```         

As above, we filter the results upregulation compared to the control of all 
significant features.

```{r, eval=FALSE}
res.full.filter <- unique(unlist(lapply(c("gefh1", "gefh1inhib"), function(x) {
   c(rownames(res.full[[x]][res.full[[x]][,"logFC"] > 0.5,]), 
     rownames(res.limit[[x]]))
})))

sig.full <- res.full[["inhib.vs.gefh1"]][
             rownames(res.full[["inhib.vs.gefh1"]]) %in% res.full.filter,]
sig.limit <- res.limit[["inhib.vs.gefh1"]][
              rownames(res.limit[["inhib.vs.gefh1"]]) %in% res.full.filter,]

nrow(sig.full) + nrow(sig.limit)
head(sig.full, n=5)
head(sig.limit, n=5)
```

This time we get significantly more proteins significantly expressed between
the groups. 

Now we plot the top result. In Figure \ref{figure/filledTop} it is nicely shown
that the protein is abundant in one condition and not abundant in 
the control or inhibitor condition at all. \emph{Note: In reality, the samples
with intensities of zero are under the detection limit and not necessarily
zero!}

```{r, eval=FALSE}
par(oma=c(4,4,4,4))
plotExpression(data.imputed, as.numeric(rownames(sig.limit[1,])))
```

To sum this up, if the features have values for each sample (are complete), 
then you want to use the model fit of the complete data set, as the 
filled in model gives an approximation. However, it is extremely useful 
to filter for genes which are up in one condition and for example
under the detection limit in the other condition.

Now it is time for some remaining stats on the data set.

Amount of complete rows before the imputation
```{r, eval=FALSE}
complete.before.imputation 
```

Amount of complete rows after the imputation
```{r, eval=FALSE}
complete.after.imputation 
```

The biggest differences in the gefh1 and the gefh1 inhibitor are detected by
looking at the data set with filled values. With our approach we 
can estimate that those are most likely not missing by chance and 
biologically might be most important. 

```{r, eval=FALSE}
stats <- t(data.frame(sapply(c("gefh1", "gefh1inhib", "inhib.vs.gefh1"), function(x) {
    return(c(nrow(res.complete[[x]]),
             nrow(res.filled[[x]]),
             nrow(res.filled[[x]])/nrow(res.complete[[x]]),
             sum(rownames(res.filled[[x]]) %in% rownames(res.complete[[x]])),
             sum(rownames(res.complete[[x]]) %in% rownames(res.filled[[x]]))))
}), row.names = c("complete", "filled", "ratio",
                  "filled.in.complete", "complete.in.filled")))

stats
```

# Frequently Asked Questions (FAQ)

No questions asked yet.

# Appendix
## Cite

If you use this package please cite 

```
TO BE ANNOUNCED
```
LIMMAS: an R package for analyzing differentially expressed proteins in quantitative label-free mass spectrometry data with multiple imputation and linear models  \ 
Schwarzl T, D'Arcangelo E, Bargary N, Malovits M, Higgins DG, **To be announced** 2015 


## Contact 

Please use the Bioconductor Mailing list for bugs or questions.

## Closing words

This package  You can also contact schwarzl@embl.de.was developed to analyse (label-free) quantitative MS data, however it can be used to analyse any data set with missing values. It was designed for a flexibel use and its focus is on the biological interpretation of the results. 

We are interested in improving the package, making the analysis of mass spectrometry data sets easy and especially well documented. Please send comments, feedback, and suggestions, or maybe a thanks to schwarzl@embl.de.

Thank you very much for using **limmas**.

## Acknowledgements

Special thanks to Alex von Kriegsheim and David Gomez with providing mass spec knowledge
and experimental data as well as Bernd Klaus and Ente Tiger for discussion.

## R Session Info

```{r} 
sessionInfo()
```

# References
